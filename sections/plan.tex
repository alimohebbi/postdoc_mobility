\subsection{Detailed research plan}
% 2000 words

%In the previous section we explained the two identified challenges of \testreuse:
In section~\ref{sec:own-research} we described the two challenges of \testreuse that we identified in our studies:
%lack of information in the GUI and uncertainty of \testreuse when semantic matching is not helpful.
Lack of textual information in the GUI, and uncertainty of current semantic matching approaches in absent of one-to-one mapping. 
%In this section we explain how objectives of the project will address these challenges. 
In this section we elaborate what are the objectives of \project project and how they address the two challenges.

\bigskip
\noindent
\textbf{O1:} to augment semantic matching with internal information other than text.  
We will use computer vision to incorporate visual information of GUI in the semantic matching.  

\bigskip
We will overcome the first challenge by using both textual and visual information available in the GUI.
In fact, users interact with apps by relying on both textual and visual information in the GUI to navigate through app windows and execute their intended functionalities. 
Additionally, most apps consider UI design best practices which emphasis on compliance of UI with users prior knowledge.  
 Thus, users expect elements with same visual cues provide similar functionalities across different applications.
For example, users expect text box with a nearby magnifier icon  provide the search functionality.
Current semantic matching approaches for \testreuse are agnostic to visual cues, while both user and developer rely on these cues to communicate possible functionalities in the app. 


%%% Maybe I need to move it to related works
\bigskip
Researchers used computer vision in many software engineering areas such as test repair~\cite{Stocco:VisualRepair:FSE:2018,Pan:Meter:TSE:2022,Xu:GUIDER:ISSTA:2021}  test generation~\cite{YazdaniBanafsheDaragh:DEEPGUI:ASE:2021,Li:Humanoid:ASE:2019,Hu:appflow:FSE:2018}, and widget recognition~\cite{zhu2021widgetrecog, white:WidgetDetection:ISSTA:2019}.
White et al. proposed an approach to improve random GUI testing by widget detection~\cite{white:WidgetDetection:ISSTA:2019}. 
%However, their approach only determines type of the widget and not its semantic.
Their approach determines the widget type like menu or text box, however, it does not provide information about semantics of widgets.
%Zhu et al. proposed an approach for identifying intent of widget and labeling them. 
Zhu et al. proposed an approach for identifying the intent of widgets and labeling them to describes the widget function~\cite{zhu2021widgetrecog}. 
However, their approach is not examined in GUI testing context. 

%%% Edite from here !!!!

\bigskip
We will create a module named \imagelabeler encompassing the encode-decoder architecture to transforms widget images to the textual description of their functionality.
We will integrate the \imagelabeler to the \testreuse~\architecture  as depicted in figure~\ref{fig:architecture} to enhance the semantic matching.
The \ede  will uses widgets coordination available in the DOM to crop images of widgets form the screenshot of the GUI.
Then, \ede queries \imagelabeler to get description of  widget images.
\ede adds the image description as another attribute of event descriptor $D$ and \matcher proceed as before.


\bigskip
%Training or fine-tuning models to generate text usually requires large train set and expensive computational resources which could be out of scope of this study. 
Training or fine-tuning models for generating text requires large train set and expensive computational resources that could be out of scope of this study. 
%If we encounter this blocking risk, we can take an alternative approach that relies on classification of images.
If this risk occurs, we will use image classification as an alternative approach.
%Similar to \texttt{AppFlow} approach, we  will  classify GUI widgets to a canonical representation that we define in advance. 
We will define canonical representations of widget  (classes) in advance and build a classifier that assigns widget to the classes.
%We assign labels to the canonical widgets that we will use to enhance the semantic matching.
We will label the canonical widgets with description of their functionalities.
%For example, we assign "back" label to the icon with a left arrow and \imagelabeler sends "back" label  to \ede for any widget that classifier consider in this class.  
For example, we assign "back" label to the icon with a left arrow.
 When \ede queries \imagelabeler, first it classifies the widget, then it returns the label of the canonical widget.
%We will use both set of canonical widgets in the literature , and we will review a large set of popular applications to identify most common widgets that has not been considered in the literature.
We will consider available canonical widgets in the literature and identify the most common widgets that are missing in the literature. 

















\bigskip
\noindent
\textbf{O2:} to use external resources of information containing patterns of functionalities. We will use LLM to translate test cases before reuse.

\bigskip
When users interact a new applications they, consider two strategies: 
a) they use their prior knowledge of using applications with similar functionalities to navigate through the new application. 
b) They leverage patterns of interactions that they leaned  in using the new application. 
Additionally, they can search in the web to understand functionalities of an application. 
LLM models can imitate both strategies of users:
a) They can recognize sequence of interactions belongs to what pattern
b)  They contain knowledge about wide variety of topics including applications functionalities.

\bigskip
We will use LLM to add more information from external resources to mitigate lack of information in the GUI.
In a simple word LLM transforms the input text to human like text. 
We can formulate problem of reusing test cases as giving sequence of text that represents source test case and contextual information as input to LLM and receive a target test case as output.  
In the \testreuse scenario, textual context is information available in the GUI of target application. 
The LLM model recognize the interaction pattern that source test belongs to, (for example, sign-in) then generates a test case for the target application with respect to the contextual information.

\bigskip
Integrating LLM in the \testreuse process requires two steps: Prompt Generation and Fine-tuning. 
%
Each prompt includes a source test case, and the contextual information of the target application. 
We consider the textual information as set of available event in each windows.
%
In Fine-tuning, we create a set of prompts and their answers.
A prompt answers is a test case that is migrated by an expert. 
% 
The output may contain events that are unreachable immediately from the previous event and  some stepping events needs to be considered to become reachable.
 Thus, we use the transformed test case as input of a conventional GUI test reuse to validate and add the stepping events to the test case. 


%%% Risk
\bigskip
A possible risk is that the generated test cases contains many unreachable events that bears too much cost on the conventional \testreuse approaches.
An alternative way to address this risk is to consider deliberate prompt engineering and presenting the contextual information such that contain graph nature of the GUI as each window in the GUI (nodes) includes multiple possible events and each events transfers the current state to another window (edges).

\bigskip
\noindent
\textbf{O3:} to make \testreuse robust in present of uncertainty. We use Reinforcement Learning to guide test reuse in absent of a matching event.

\bigskip
In the cases that a matching event is unavailable in the current state, \testreuse approaches use the GUI model of the application to create stepping event and reach an state where the matching event is available. 
\testreuse approaches create the GUI model  by static analysis of the application and complete it as they go through the \testreuse process. 
But, the GUI model is often incomplete and events may remain unreachable. 
Other times semantic matching cannot find any matching event neither in the current state nor in the GUI model. 
In such conditions that semantic matching is ineffective, \atm and \adaptdroid consider a random action, however, that is not always helpful and may result in many false positives. 
We will use Reinforcement Learning in the \testreuse to handle the uncertain condition.

\bigskip
Training a Reinforcement Learning agent requires defining states, reward function, and actions.
We define the states as set of textual attributes of widget available in the GUI state and we consider the reward function the same as the reward function of AutoBlackBox~\cite{Mariani:Autoblacktest:ICSE:2011} which is  how much the new state is different in comparison to the previous states. 
%
We use Reinforcement Learning in two stages: 
First, we use RL to explore the GUI before start of \testreuse process. 
Since the reward functions values  novel states it the agent will explore GUI effectively and we can create a more comprehensive model of the GUI automatically. 
Second, in conditions that semantic matching is  ineffective, the \testreuse approach queries the agent for the next actions.
Our intuition is that the agent will transform the state to novel state that might contains an event match. 
We will limit number of consequent actions that agent takes to handle conditions that a matching event do not exist at all.


