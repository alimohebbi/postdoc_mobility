\subsection{Detailed research plan}
% 2000 words
\label{sec:detailed-plan}

%In section~\ref{sec:own-research}, we described the two challenges of \testreuse that we identified in our studies:
%Lack of textual information in the GUI and uncertainty of current semantic matching approaches in the absence of one-to-one mapping. 
In this section, we explain how objectives of the \project project address the two challenges of \testreuse: Lack of textual information in the GUI, and ineffectiveness of  \selector in the absence of comprehensive \tam. 


\bigskip 
\noindent
\textbf{O1:} 
to augment semantic matching with internal information other than text.  
We will use computer vision to incorporate visual information of GUI in the semantic matching.  

\smallskip
We will overcome the first challenge by using  textual and visual information available in the GUI.
Users interact with apps by relying on  textual and visual information in the GUI to execute their intended functionalities. 
Additionally, most apps consider UI design best practices, which emphasize compliance of UI with users prior knowledge.  
Thus, users expect elements with the same visual cues to provide similar functionalities across different applications.
For example, users expect a text box with a nearby magnifier icon provides the search functionality.
Current semantic matching approaches for \testreuse are agnostic to visual cues, while both user and developer rely on these cues to communicate possible functionalities in the app. 
Xu et. al.  study concluded textual and visual information in the GUI are complementary~\cite{Xu:GUIDER:ISSTA:2021}.


%\smallskip 
%\ali{Maybe I need to move it to related works.}
%Researchers used computer vision in many software engineering areas, such as test repair~\cite{Stocco:VisualRepair:FSE:2018,Pan:Meter:TSE:2022,Xu:GUIDER:ISSTA:2021},  test generation~\cite{YazdaniBanafsheDaragh:DEEPGUI:ASE:2021,Li:Humanoid:ASE:2019,Hu:appflow:FSE:2018}, and widget recognition~\cite{zhu2021widgetrecog, white:WidgetDetection:ISSTA:2019}.
%White et al. proposed an approach that improves random GUI testing by widget detection~\cite{white:WidgetDetection:ISSTA:2019}. 
%Their approach determines the widget type, such as the menu or text box. However, it does not provide information about the semantics of widgets.
%Zhu et al. proposed an approach for identifying the intent of widgets and labeling them to describe the widget function~\cite{zhu2021widgetrecog}. 
%However, their approach is not examined in the GUI testing context. 


\smallskip
%We will create a module named \imagelabeler encompassing the encode-decoder architecture to transform widget images to the textual description of their functionality.
We will create a component named \imagelabeler to transform widget images to the textual description of their functionality.
We will integrate the \imagelabeler into the \testreuse~\architecture, as depicted in figure~\ref{fig:architecture}, to enhance the semantic matching.
The \ede  will use widgets coordination available in the DOM to crop images of widgets from the screenshot of the GUI.
Then, the \ede queries the \imagelabeler to get the description of  widget images.
\ede adds the image description as another attribute of event descriptor $D$, and \matcher proceeds as before.


\smallskip
Training or fine-tuning models for generating text requires a large train set and expensive computational resources that could be out of the scope of this study. 
If this risk occurs, we will use image classification as an alternative approach.
We will define canonical representations of widgets  (canonical widgets) and build a classifier that assigns widgets to the canonical classes.
We will label the canonical widgets with a relevant description.
For example, we assign the "back" label to the "$\leftarrow$" icon.
\ede sends a query to the \imagelabeler to classify the widgets and receive a label for them.
%We will consider available canonical widgets in the literature and identify the most common widgets that are missing in the literature. 



\bigskip 
\noindent
\textbf{O2:} to augment \testreuse with external resources of information.
We will use LLM to translate source test cases and migrate the translated test cases.

\smallskip
We will  address the first challenge of \testreuse by incorporating external information available in the LLM models.
When users interact with new applications, they consider two strategies to use the apps:
\begin{inparaenum}[(i)]
\item they recall prior  interaction patterns of  applications with similar functionalities
\item They search the web to learn about the functionalities of an application.
\end{inparaenum}
LLM models can imitate both strategies:
First, LLM models can recognize patterns in a sequence of text~\cite{Pandey:TransDPR:EMSE:2023}, and GUI interactions can be  represented as a sequence of text. 
Second, LLM models embed knowledge of a variety of topics including application functionalities.
Additionally, LLM models can understand the semantic similarity of text. 
Thus, transforming test cases by LLM can benefit from both incorporating external information and making the source test case more compatible with the target application vocabulary.


%
%\smallskip
%We will formulate \testreuse as a prompt that its answer is a migrated test case for the target application.
%The prompt contains a source test case and the information in the GUI of the source and target application that provides context for the prompt. 
%The LLM model recognizes the interaction pattern of the source test case (For example, the Sign-In pattern), and then generates a test case with respect to the contextual information.


\smallskip
Using LLM for specialized tasks such as \testreuse requires two steps: Prompt Generation and Fine-tuning. 
We will generate prompts that include source test cases as sequences of events and contextual information as textual descriptors of events in the source test case and the target application.
We will tune the LLM models with a set of prompts and their answers, which is the ground truth for migrating the source test case in the prompt.
The tuned LLM model recognizes the interaction pattern of the source test case (For example, the Sign-In pattern), and then generates a test case with respect to the contextual information.
The generated test case by LLM may contain events that require adding stepping events to be reachable immediately from the previous events.
 Thus, we validate and complete the transformed test case by reusing it as input of the \selector, as depicted in Figure~\ref{fig:architecture}. 



%%% Risk
\smallskip
A possible risk is that the transformed test case by LLM contains too many unreachable events, which reduces the quality of the generated test cases.
The reason is the defined prompts does not contain enough information  for the LLM model to infer what actions are available in each state.
If this risk occurs, we can carefully design a more complicated prompt that embeds the \tam and conveys permissible evens after each selected event.




\bigskip 
\noindent
\textbf{O3:} to make \testreuse robust in decision making. We will use Reinforcement Learning to build a comprehensive \tam that guides \testreuse in the absence of matching events. 

\smallskip
When a matching event is unavailable in the current state, the \selector queries the \tam to find a path to the state where the matching event exists.
%\testreuse approaches initially build the \tam by static analysis of the target application and complete the model as they observe new states while progressing in migration of the source events. 
Usually, the \tam is incomplete and misses many states. %and actions that lead to them.
If \selector do not find an event, it takes random actions.  However, a matching event may remain unreachable, and the random action reduces quality of test. 


\smallskip
We will use Reinforcement Learning in the \testreuse to enhance the \tam and take actions where semantic matching is ineffective.
Training a Reinforcement Learning agent requires defining  states, reward functions, and actions.
We consider the RL states as the set of textual attributes of widgets in the GUI states, the reward function as how much the new state differs from previous ones, and actions as GUI events.

\smallskip 
Figure~\ref{fig:architecture} shows the position of the \rlaganet in the \testreuse~\architecture.
We will use the \rlaganet in two stages: 
First, we will use the agent to explore the GUI and update the \tam before \testreuse.
The reward function values novel states. Thus,  it will explore the GUI effectively and dynamically build a  comprehensive \tam.
Second, when semantic matching is ineffective, the \selector queries the \rlaganet for the next event.
Our intuition is that if a matching event is unavailable in the \tam, we might find the matching event in a novel state and the RL agent navigates to the state.
%We will limit the number of consecutive queries from the agent to handle umatchable events in the source test case.


