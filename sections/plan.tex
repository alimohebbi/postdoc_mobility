\subsection{Detailed research plan}
% 2000 words

In section~\ref{sec:own-research} we described the two challenges of \testreuse that we identified in our studies:
Lack of textual information in the GUI, and uncertainty of current semantic matching approaches in absent of one-to-one mapping. 
In this section we elaborate what are the objectives of \project project and how they address the two challenges.

\bigskip
\noindent
\textbf{O1:} to augment semantic matching with internal information other than text.  
We will use computer vision to incorporate visual information of GUI in the semantic matching.  

\bigskip
We will overcome the first challenge by using both textual and visual information available in the GUI.
In fact, users interact with apps by relying on both textual and visual information in the GUI to navigate through app windows and execute their intended functionalities. 
Additionally, most apps consider UI design best practices which emphasis on compliance of UI with users prior knowledge.  
 Thus, users expect elements with same visual cues provide similar functionalities across different applications.
For example, users expect text box with a nearby magnifier icon  provide the search functionality.
Current semantic matching approaches for \testreuse are agnostic to visual cues, while both user and developer rely on these cues to communicate possible functionalities in the app. 



\bigskip
\ali{Maybe I need to move it to related works.}
Researchers used computer vision in many software engineering areas such as test repair~\cite{Stocco:VisualRepair:FSE:2018,Pan:Meter:TSE:2022,Xu:GUIDER:ISSTA:2021}  test generation~\cite{YazdaniBanafsheDaragh:DEEPGUI:ASE:2021,Li:Humanoid:ASE:2019,Hu:appflow:FSE:2018}, and widget recognition~\cite{zhu2021widgetrecog, white:WidgetDetection:ISSTA:2019}.
White et al. proposed an approach to improve random GUI testing by widget detection~\cite{white:WidgetDetection:ISSTA:2019}. 
Their approach determines the widget type like menu or text box, however, it does not provide information about semantics of widgets.
Zhu et al. proposed an approach for identifying the intent of widgets and labeling them to describes the widget function~\cite{zhu2021widgetrecog}. 
However, their approach is not examined in GUI testing context. 


\bigskip
We will create a module named \imagelabeler encompassing the encode-decoder architecture to transforms widget images to the textual description of their functionality.
We will integrate the \imagelabeler to the \testreuse~\architecture  as depicted in figure~\ref{fig:architecture} to enhance the semantic matching.
The \ede  will uses widgets coordination available in the DOM to crop images of widgets form the screenshot of the GUI.
Then, \ede queries \imagelabeler to get description of  widget images.
\ede adds the image description as another attribute of event descriptor $D$ and \matcher proceed as before.


\bigskip
Training or fine-tuning models for generating text requires large train set and expensive computational resources that could be out of scope of this study. 
If this risk occurs, we will use image classification as an alternative approach.
We will define canonical representations of widget  (classes) in advance and build a classifier that assigns widget to the classes.
We will label the canonical widgets with description of their functionalities.
For example, we assign "back" label to the icon with a left arrow.
 When \ede queries \imagelabeler, first it classifies the widget, then it returns the label of the canonical widget.
We will consider available canonical widgets in the literature and identify the most common widgets that are missing in the literature. 




\bigskip
\noindent
\textbf{O2:} to augment \testreuse with external resources of information.
We will use LLM to transform source test cases and migrate the transformed.

\bigskip
When users interact with new applications they consider two strategies to effectively use the apps:
\begin{inparaenum}[a)]
\item they recall prior  interaction patterns of  applications with similar functionalities
\item They search in the web to learn about functionalities of an application.
\end{inparaenum}
LLM models can imitate the both strategies:
First, LLM models can recognize patterns in sequence of text and  GUI interactions can be textually represented as sequence of actions. 
Second, LLM models embed knowledge of variety of topics including applications functionalities.
Additionally, LLM models can understand semantic similarity of text. 
Thus, transforming test cases by LLM can benefit from both incorporating external information, and helping semantic matching by making the source test case more compatible with target application terminology.



\bigskip
We will  address the first challenge of \testreuse, lack of information in the GUI, by incorporating external information available in the LLM models.
We can formulate \testreuse as a prompt that its answer is a migrated test case for the target application.
The prompt contains a source test case and the information in the GUI of source and target application that provides context of the prompt. 
The LLM model recognizes the interaction pattern of the source test case (For example, Sign-In pattern), then generates a test case with respect to the contextual information.


\bigskip
Using LLM for specilized tasks such as \testreuse requires two steps: Prompt Generation and Fine-tuning. 
We will generate prompts that include source test cases as sequence of events and the contextual information as textual descriptors of events in the source test case and the target application.
We will tune the LLM models with a set of prompt and their answers, which is ground truth for migrating the source test case in the prompt.
The generated test case by LLM may contain events that are unreachable immediately from the previous events and they require stepping events.
 Thus, we validate and complete the transformed test case by reusing it as input of \selector as depicted in figure~\ref{fig:architecture}. 



%%% Risk
\bigskip
A possible risk is that the transformed test case by LLM contain too many unreachable events, which reduces the quality of the generated test cases.
The reason is the defined prompt do not contain enough information  for LLM model to infer what actions are available in each state.
If this risk occurs, we can carefully design a more complicated prompt that embeds \tam and convey permissible next evens after each selected event.




\bigskip
\noindent
\textbf{O3:} to make \testreuse robust in present of uncertainty. 
We will use Reinforcement Learning to guide \testreuse in absent of one-to-one matching.

\bigskip
When a matching event is unavailable in the current state, \selector queries the \tam to find a path to the state where the matching event exist.
\testreuse approaches initially build the \tam by static analysis of the target application and they complete the model as they observe new states while they  progress in migrating the source events.
Usually, the \tam remains incomplete and misses many states and actions that leads to them.
In the conditions where semantic matching become ineffective, \atm and \adaptdroid take a random action, but a matching event may remain unreachable and the random action increases the false positives rate. 


\bigskip
We will use Reinforcement Learning in the \testreuse to make the \tam more complete and take actions in uncertain conditions.
Training a Reinforcement Learning agent requires defining  states, reward function, and actions.
%We define the states as set of textual attributes of widget available in the GUI state and we consider the reward function the same as the reward function of AutoBlackBox~\cite{Mariani:Autoblacktest:ICSE:2011} which is  how much the new state is different in comparison to the previous states. 
We define the RL states as the set of textual attributes of widgets in the GUI states, reward function as how much the new state is different from previous ones, and actions as GUI events.

\bigskip
Figure~\ref{fig:architecture}, shows how the RL agent will be located in the \testreuse~\architecture.
%We use Reinforcement Learning in two stages: 
We will use a RL agent in two stages: 
%First, we use RL to explore the GUI before start of \testreuse process. 
%First, we use RL to explore the GUI before start of \testreuse process. 
First, we will use the agent to explore the GUI and update the \tam before \testreuse.
%Since the reward functions values  novel states it the agent will explore GUI effectively and we can create a more comprehensive model of the GUI automatically. 
The reward function of the agent values novel states, thus it will explore the GUI effectively and builds a more comprehensive \tam.
%Second, in conditions that semantic matching is  ineffective, the \testreuse approach queries the agent for the next actions.
Second, when semantic matching is ineffective, the \selector queries the agent for the next event.
%Our intuition is that the agent will transform the state to novel state that might contains an event match. 
%Our intuition is that the agent will move  to a novel state that might contains a matching event. 
Our intuition is that if a matching event is unavailable in the current state or unreachable from \tam, we might find the matching event in a novel state.
We will limit number of consecutive queries from the agent to handle conditions where a matching event do not exist at all.


